# Machine Learning

## MATLAB教程

### 基本操作

基本的数据运算，包括加减乘除。

逻辑运算：

```matlab
> 1 == 2	% flase
ans	= 0
> 1 ~= 2
ans = 1
> 1 && 0;
```

注：在matlab中`%`号表示注释，不等于用`~=`来判断，`;`用于阻止结果的输出，如上面代码的最后一个命令，加上`;`后无输出结果。

#### 打印输出变量

```matlab
> a = pi;
> a
a =	3.1416


> disp(a)
 3.1416
 
 
> disp(sprintf('2 decimals: %0.2f', a))
2 decimals: 3.14
```

最后一项命令为C语言的旧式语法，`sprintf()`命令返回一个字符串，再通过`disp()`命令输出。

#### 控制打印长度

```matlab
> 1 = pi;
> format long
a =	3.14159265358979
> format short
> a
a = 3.1416

```

#### 矩阵

```matlab
> A = [1 2; 3 4; 5 6]
A =
	1	2
	3	4
	5	6
```

其中`;`表示矩阵切换到下一行

```matlab
> v = 1:0.1:2
v =

  列 1 至 8

    1.0000    1.1000    1.2000    1.3000    1.4000    1.5000    1.6000    1.7000

  列 9 至 11

    1.8000    1.9000    2.0000
```

上述的命令的具体含义为，从1为起点，步长为0.1，一直增加到2的一个行向量，**如果不设置步长默认为1。**

##### 生成特殊矩阵

```matlab
>> A = ones(2,3)

A =

     1     1     1
     1     1     1
>> zeros(1,3)

ans =

     0     0     0
>> rand(3,3)

ans =

    0.8147    0.9134    0.2785
    0.9058    0.6324    0.5469
    0.1270    0.0975    0.9575
```

在`rand(3,3)`中，会生成一个3x3的矩阵，里面的元素全是位于0到1之间的随机值，`randn()`可以生成满足**高斯分布(正态分布)**的随机值，例如：

```matlab
>> w = -6 + sqrt(10) * (randn(1,10000));
>> hist(w);
```

结果为：

![p1](./res/p1.png)

```matlab
>> eye(4)
ans =

     1     0     0     0
     0     1     0     0
     0     0     1     0
     0     0     0     1
```

`eye()`表示生成单位矩阵

### 移动数据

```matlab
>> A = [1 2; 3 4; 5 6];
>> size(A)
ans =
	3	2
>> v = [1 2 3 4]
>> length(v)
ans = 4	%将会返回最大维度的大小，一般用于求向量的大小，而不是用于矩阵
>> pwd
ans = D://.....	%显示当前路径
>> cd 'C:\Users'
```

载入文件：

```matlab
>> load xxxx.txt
>> load('xxxx.txt')
>> who	%可以显示出当前所有的变量
>> whos %可以显示出当前所有变量的详细信息
>> clear X	%删除X变量
>> clear	%删除所有变量
>> save xxx.mat V	%保存文件，把变量V存储为xxx.mat的文件
```

定位元素：

```matlab
>> A = [1 2; 3 4; 5 6];
>> A(3,2)
ans =	6
>> A(2,:)	%":"表示该行或列所有的元素
ans =
	3	4
>> A([1 3], :)	%获取第一行和第三行的所有元素
ans =
	1	2
	5	6
>> A = [A, [100;200;300]]	%可以在右边加一列新的矩阵
A =
     1     2   100
     3     4   200
     5     6   300
>> A(:)	%把A中的所有元素放到一列中
ans =
     1
     3
     5
     2
     4
     6
   100
   200
   300
```

### 计算数据

* 矩阵的乘法

  ```matlab
  >> A = [1 2; 3 4; 5 6];
  >> B = [11 12 13; 14 15 16];
  >> A*B
  ans =
  
      39    42    45
      89    96   103
     139   150   161
  ```

  上面的为矩阵的直接乘法，若要单独对元素进行操作则应该使用`.`

  ```matlab
  >> A = [1 2; 3 4; 5 6];
  >> B = [11 12; 13 14; 15 16];
  >> A.*B
  ans =
  
      11    24
      39    56
      75    96
  ```

  使用`.`可以对元素进行操作，不限于乘法。

* 计算绝对值：

  使用`abs()`命令可以求绝对值

* 矩阵转置：

  ```matlab
  >> A = [1 2; 3 4; 5 6]
  A =
       1     2
       3     4
       5     6
  >> A'
  ans =
       1     3     5
       2     4     6
  ```

  使用标准引号`'`可以求得矩阵的转置

* 逻辑比较：

  ```matlab
  >> a = [1 15 2 0.5];
  >> a < 3
  ans =
     1   0   1   1
  ```

  符合返回1，不符合返回0
  
* `magic()`函数

  `magic(n)`函数将会返回一个n阶的幻方矩阵，该矩阵的特点是每行每列以及对角线上的元素和都相等。

  ```matlab
  >> magic(3)
  ans =
       8     1     6
       3     5     7
       4     9     2
  ```
  
* `find()`函数

  ```matlab
  >> a = [1 15 2 0.5];
  >> find(a < 3 )
  ans =
       1     3     4
  ```

  将会返回符合条件的索引
  
  ```matlab
  >> A = magic(3)
  ans =
       8     1     6
       3     5     7
       4     9     2
  >> [r c] = find(A>=7)
  r =
       1
       3
       2
  c =
       1
       2
       3
  ```
  
  上述代码返回了符合条件的行和列的索引，比如`A(1,1)`大于等于7，`A(3,2)`也大于等于7。

* 求和：

  ```matlab
  >> a = [1 15 2 0.5];
  >> sum(a)
  ans =
     18.5000
  ```

  使用`sum()`函数可以求一个向量中的所有元素之和

  另外有：

  ```matlab
  >> A = magic(9)
  A =
      47    58    69    80     1    12    23    34    45
      57    68    79     9    11    22    33    44    46
      67    78     8    10    21    32    43    54    56
      77     7    18    20    31    42    53    55    66
       6    17    19    30    41    52    63    65    76
      16    27    29    40    51    62    64    75     5
      26    28    39    50    61    72    74     4    15
      36    38    49    60    71    73     3    14    25
      37    48    59    70    81     2    13    24    35
  >> sum(A,1)
  ans =
     369   369   369   369   369   369   369   369   369
  ```

  `sum(A,1)`返回了每列的求和，及把第1维度即每行的元素求和。同理`sum(A,1)`返回每列的求和，及把第2维度即每列的元素求和。

  另外注意，**如果不注明维度，则默认求第一维度即每列的和。**

  如何求对角线的和：

  我们先使用`A.*eye(9)`来使得除了对角线外的所有元素均为0

  ```matlab
  >> A = magic(9);
  >> A.*eye(9)
  ans =
      47     0     0     0     0     0     0     0     0
       0    68     0     0     0     0     0     0     0
       0     0     8     0     0     0     0     0     0
       0     0     0    20     0     0     0     0     0
       0     0     0     0    41     0     0     0     0
       0     0     0     0     0    62     0     0     0
       0     0     0     0     0     0    74     0     0
       0     0     0     0     0     0     0    14     0
       0     0     0     0     0     0     0     0    35
  ```

  接下来只需求这个矩阵的所有元素之和：

  ```matlab
  >> sum(A.*eye(9), 'all')
  ans =
     369
  ```

  我们还可以使用`flipud()`命令来使单位矩阵垂直翻转来求另一条对角线的和：

  ```matlab
  >> A = magic(9);
  >> A.*flipud(eye(9))
  ans =
       0     0     0     0     0     0     0     0    45
       0     0     0     0     0     0     0    44     0
       0     0     0     0     0     0    43     0     0
       0     0     0     0     0    42     0     0     0
       0     0     0     0    41     0     0     0     0
       0     0     0    40     0     0     0     0     0
       0     0    39     0     0     0     0     0     0
       0    38     0     0     0     0     0     0     0
      37     0     0     0     0     0     0     0     0
  >> sum(A.*flipud(eye(9)), 'all')
  ans =
     369
  ```

  

  

* 求积：

  ```matlab
  >> a = [1 15 2 0.5];
  >> prod(a)
  ans =
  	15
  ```

* 取整：

  `floor()`函数可以对数字进行向下取整，`ceil()`函数可以对数字进行向上取整。

* `max()`函数

  * 对向量:

    ```matlab
    >> a = [1 15 2 0.5];
    >> max(a)
    ans = 
  	  15
    ```
    上述例子的`max()`函数返回了一个向量之中的最大值，如果要获取它的索引则有：

    ```matlab
    >> a = [1 15 2 0.5];
    >> [val, ind] = max(a)
    val =
        15
    ind =
         2
    ```
    其中`val`返回了最大元素的值，`ind`返回了其索引
    
  * 对矩阵：

    ```matlab
    >> A = magic(3)
    ans =
         8     1     6
         3     5     7
         4     9     2
    >> max(A,[],1)
    ans =
         8     9     7
    ```
  
    上面的结果返回了每列元素的最大值，后面的1表示对第一维度进行遍历，从第1行到第n行进行比较，返回最大值，从而求出每列最大的元素，同理有`max(A, [], 2)`来求得每行元素的最大值。
  
    当然要记得，**如果对一个矩阵进行如同`max(A)`一般的操作，则默认返回以第一维度求和即每列的最大值。**
  
    如果你想求得一个矩阵中最大的元素，则可以用以下方法：
  
    ```matlab
    >> max(max(A));
    >> max(A, [], 'all');
    ```
  
* 求逆矩阵：
  
  ```matlab
  >> pinv(A)
  ```
  
  使用`pinv()`来求矩阵的逆矩阵
  
### 数据绘制

```matlab
>> t=[0:0.01:1];
>> y1=sin(2*pi*4*t);
>> plot(t,y1)
```

上述命令可以简单的绘制出一个正弦函数：

![数据绘制](./res/p2.png)

我们接着输入：

```matlab
>> y2=cos(2*pi*4*t)
>> plot(t,y2);
```

这时你会发现，matlab抹除了之前绘制的sin函数，再绘制出了cos函数，要想把两个曲线绘制到同一个图上，我们可以这样做：

```matlab
>> plot(t,y1);
>> hold on;
>> plot(t,y2,'r');	%将曲线颜色设置为红色
```

`hold on`可以保留之前绘制的内容然后接着绘制新的内容：

![数据绘制3](./res/p3.png)

我们还可以在图上添加其他的数据：

```matlab
>> xlabel('time')	%给x轴加上标签
>> ylabel('value')	%给y轴加上标签
>> legend('sin','cos')	%在图上标明曲线的名字
>> title('my plot')	%给图取名字
```

![数据绘制4](./res/p4.png)

保存图像：

```matlab
>> print -dpng 'my Plot.png'
```

这会在当前路径保存图片，另外`close`可以把图像关闭

在两张图上绘制：

```matlab
>> figure(1);
>> plot(t,y1);
>> figure(2);
>> plot(t,y2);
```

![5](./res/p5.png)

这样我们就得到了两个图

在一个图里画多个独立的曲线：

```matlab
>> subplot(1,2,1);	%这个语句前两个参数表示把图分为1*2的格子，最后一个参数表示使用第一个格子
>> plot(t,y1);
```

![6](./res/p6.png)

```matlab
>> subplot(1,2,2);
>> plot(t,y2);
```

![7](./res/p7.png)

调整轴的范围：

```matlab
>> axis([0.5 1 -1 1])	%前两个调整x轴的范围，后两个调整y轴的范围
```

可视化矩阵：

```matlab
>> A = magic(5)
>> imagesc(A)
```

![8](./res/p8.png)

`colorbar`可以在旁边加一个颜色对照表：

![10](./res/p9.png)

## $logistic$回归

一般用于分类问题

### 分类

#### 二分类

此类分类只有两类，换言之该分类只有两种状态。

### 假设陈述

因为在分类问题中，尤其是在二分类中，状态通常只有0和1两种，但是在线性回归中，$h_\theta(x)$会出现大于1的情况，所以通常不适合于分类问题，所以现在需要一种新的概述，使$h_\theta(x)$处于0到1之间。

在线性回归中我们的$h_\theta(x)$为：
$$
h_\theta(x)=\theta^Tx
$$
在$logistic$回归中，我们对$h_\theta(x)$做出改进，令$h_\theta(x)=g(\theta^Tx)$，其中：
$$
g(x)={1\over1+e^{-x}}
$$
我们将两个方程结合起来，就得到了新的$h_\theta(x)$：
$$
g(x)={1\over1+e^{-\theta^Tx}}
$$
这个函数一般被称为$sigmoid$函数，其图形为：

![](./res/p10.png)

有了这个假设，我们只需要一组$\theta$来拟合我们的数据。

#### 模型的解释：

$h_\theta(x)$估计了输入$x$时$y=1$的概率



### 决策界限

$sigmoid$函数已经决定了输入$x$时$y=1$的概率，当$x$大于$0$的时候$h_\theta(x)$大于0.5，表示$y=1$，反之则为$y=0$。

现在问题就转换为拟合$\theta^T$使在$X$时的$\theta^{T}X$大于$0$，如下图。

![](/res/p11.png)

图中的点集为数据集，一共有两个特征变量，得出$h_\theta(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)$

假设我们已经通过方法把$\theta$都拟合出了，得出$h_\theta(x)=g(-3+x_1+x_2)$

当我们预测$y=1$时，使$-3+x_1+x_2>0$，根据我们高中学过的线性回归，我们就可以在图上画出一条直线，在直线之上的点都表示$y=1$，这条线就被称为决策界限。

### 代价函数

在上面的问题中我们谈到了决策界限的定义，但是决策界限是基于已经拟合出来的$\theta$来决定了，现在我们的问题来到了如何拟合$\theta$，要拟合$\theta$首先需要把代价函数求出来。

在线性回归中我们的代价函数为：
$$
J(\theta )=\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2   
$$
但是在$logistic$回归中，因为$h_\theta(x)$的不同，$J(\theta)$为非凸函数，存在多个局部最优解，不可以使用梯度下降法拟合，因此我们将$J(\theta)$设为：
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x),y)
$$
其中：
$$
Cost(h_\theta(x),y)=\begin{cases}
-log(h_\theta(x))&if&y=1\\
-log(1-h_\theta(x))&if&y=0
\end{cases}
$$
当$y=1$时：

![](./res/p12.png)

代价函数如图所示，当估计的$h_\theta(x)$为1时，实际上的$y$也为$1$，可以很好的拟合，因此我们的代价在图中为0；

当$h_\theta(x)$估计为0时，实际上为$y=1$，与实际不符，所以图中的代价为$\infty$

反之在下图$y=0$时也成立

![](./res/p13.png)

### 简化代价函数以及梯度下降

在上面我们得出了$logistic$回归的代价函数，我们现在可以来将它化简为：
$$
Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
$$
此式也可以表示为$logistic$回归的代价函数

现在有：
$$
J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}ylog(h_\theta(x))+(1-y)log(1-h_\theta(x))]
$$
To fit parameters $\theta$:

We need to minimize $J(\theta)$

We use Gradient Descent to minimize $J(\theta)$
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
$$
重复上式，其中
$$
\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$


### 高级优化

我们可以使用matlab中自带的一些优化算法来拟合出$J(\theta)$的$\theta$，下面举个例子：

Example：
$$
\theta=\begin{bmatrix}
 \theta_1\\
\theta_2
\end{bmatrix}\\
J(\theta)=(\theta_1-5)^2+(\theta_2-5)^2\\
\frac{\partial}{\partial\theta_1}J(\theta)=2(\theta_1-5)\\
\frac{\partial}{\partial\theta_2}J(\theta)=2(\theta_2-5)
$$
在matlab中我们可以写这样一个函数：

```matlab
function [jVal, gradient]
			= costFunction(theta)
	jVal = (theta(1)-5)^2 + (theta(2)-5)^2;	//为代价函数
	gradient = zeros(2,1);					//梯度值
	gradient = 2*(theta(1)-5);
	gradient = 2*(theta(2)-5);
```

然后在终端中输入：

```matlab
option = optimset('GradObj', 'on', 'MaxIter', '100');	//优化设置
initialTheta = zeros(2,1);								//初始化theta
[optTheta, functionVal, exitFlag]
	= fminunc(@costFunction, initialTheta, option);
```



### 多元分类问题

我们现在要分多个类别，比如分类邮件为亲人、同事、广告三个，这就涉及到多元分类问题。

我们将上面的类别分别设为1、2、3，多个类别时我们的点集可能如下：

![](./res/p14.png)

我们可以将2、3看为负类，将1看为正类，得到y=1时的$h_\theta^{(1)}(x)$

这样我们就可以得到三个分类器，对应三个不同的类别，这样就实现了多元分类问题。

## 过拟合问题

### 过拟合

过拟合($overfitting$)：也称为过学习，他的直观表现是在学习出的模型在训练集上表现良好，但在实际问题出表现不好，泛化性差。

![](./res/15.png)

上图表现出的算法在以给定的训练集上表现良好，但是在实际问题中不能很好的应用，泛化性差，这种问题我们称之为过拟合(通过样本点且波动程度太大)。

一般过拟合会在变量过多的时候出现。

### 如何解决过拟合

1. 尽量减少所选的变量的数量。（缺点是可能舍去与预测有关的变量）
2. 正则化：保持变量数量不变，把$\theta_j$减小。



### 变化代价函数

减小$\theta_j$意味着得到的模型更简单，泛化性更强。

所以在代价函数后面加上以下公式即可减小$\theta_j$
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$
其中$\lambda$为正则化系数，负责控制$\theta$的变化大小。



### 线性回归的正则化

上面我们提到正则化就是修改**代价函数**使之对$\theta_j$进行减小以是模型更为简单。修改之后的代价函数为：
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{m}\theta_j^2]
$$


下面是线性回归的正则化

重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\
\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$
其中对$\theta_0$不进行修改是约定俗成的规矩，对$\theta_0$是否进行正则化对结果的影响不是很大。其中上面对$\theta_j$的梯度下降可写为：
$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$


其中$(1-\alpha\frac{\lambda}{m})$是一个非常接近但小于1的数字。

正规方程的正则化：
$$
\theta=(X^TX+\lambda
\begin{bmatrix}
{0}&&&\\
&{1}&&\\
&&{1}&\\
&&&{1}\\
&&&&{\ddots}\\
&&&&&{1}
\end{bmatrix})^{-1}X^Ty
$$


$logistic$的正则化
$$
J(\theta)=[-\frac{1}{m}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2
$$


重复：
$$
\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$

## 神经网络

### 背景

人们想要创造出一种可以模仿人类大脑结构的算法。

### 大脑中的神经元

![](./res/17.png)

胞体负责接受信号，轴突负责发送信号。

### 神经元模型：Logistic unit

![](./res/18.png)

上面为神经网络的一个神经元，相当于神经元接受一些输入，经过处理后再输出计算的结果$h_\theta(x)$，其中$h_\theta(x)$为$\frac{1}{1+e^{-\theta^Tx}}$

![](./res/19.png)

一般来说可以将$x_1,x_2,x_3$作为输入数据，我们通常会添加一个$x_0=1$作为偏置单元同样作为输入

刚才描述了一个以$sigmoid$函数作为激活函数($activation function$)的人工神经元，代表一个神经元，神经网络实际上就是很多个神经元组合起来的模型：![](./res/20.png)

第一层称为输入层，最后一层输出层，其他层都称为隐藏层。

上图中的$a_1^{(2)}$表示第2层的第一个激活项(activation)

$\Theta^{j}$表示控制第j层到j+1层的映射的权重矩阵，比如第1层经过$\Theta^1$的处理后才变成了第2层的数据。

![](./res/21.png)

这边是具体的权重矩阵与激活项的计算细节。

我们定义上面的$\Theta^{(1)}_{10}x_0+\Theta^{(1)}_{11}x_1+\Theta^{(1)}_{12}x_2+\Theta^{(1)}_{13}x_3=z^{(2)}_1$

即有：
$$
a^{(2)}_1=g(z^{(2)}_1)
$$

$$
x=\begin{bmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{bmatrix}
&&&&z^{(2)}=\begin{bmatrix}
z^{(2)}_1\\
z^{(2)}_2\\
z^{(2)}_3
\end{bmatrix}
$$

$$
z^{(2)}=\Theta^{(1)}x
$$

### 举例

假设我们有两个输入值：

![](./res/22.png)

表示只有当$x_1,x_2$都为1或0时结果为假，现在我们需要一个模型来学习它。

我们先介绍逻辑**与**的神经网络结构：

![](./res/23.png)

上图中的$+1$为偏置单元，我们得到的$\Theta$为：-30，20，20，这样就有了
$$
h_\Theta(x)=g(-30+20x_1+20x_2)
$$
这样就简单表示了**与**运算。可以自己画一个真值表来验证。

我们再来接受逻辑**或**的神经网络结构：

![](./res/24.png)

得到的$\Theta$为-10，20，20

逻辑**非**的结构;

![](./res/25.png)

现在我们把这三个结构结合起来：

![](./res/26.png)

其中红色的节点表示$x_1,x_2$都为1时，蓝色表示$x_1,x_2$都为0时，绿色的OR运算表示红色和蓝色成立一种时就输入肯定答案。

这样神经网络的作用就体现出来了，神经网络可以将复杂的算法简单化，把复杂的算法在每一层用简单的算法表示出来。

### 多分类问题

对于多分类问题，我们可以让$h_\theta(x)$的输出为矩阵。比如$\begin{bmatrix}1\\0\\0\end{bmatrix}$表示第一种结果，$\begin{bmatrix}0\\1\\0\end{bmatrix}$表示第二种结果，$\begin{bmatrix}0\\0\\1\end{bmatrix}$表示第三种结果。

### 代价函数

定义：

$L$为神经网络的层数

$s_l$为第l层神经网络的结点数量，特别定义$K$为输出层的结点数量

**神经网络的代价函数**：
$$
J(\Theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$
前面的$k$的和表示多分类时，多个输出结点的输出结果之和，当$K=1$时可忽略。

后半项的作用为正则化，其中不正则化下标为$0$的$\Theta$，这点于$logistic$回归一致。

### **反向传播算法**

回顾我们的神经网络的代价函数，我们的目标是最小化$J(\Theta)$，我们需要计算：

* $J(\Theta)$
* $\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

我们定义：$\delta_j^{(l)}$表示第$l$层第$j$个结点的误差。

举个例子(layer L = 4)：

$\delta^{(4)}=a^{(4)}-y$

$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)}·*g'(z^{(3)})$

$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)}·*g'(z^{(2)})$

其中$g'(z)=a·*(1-a)$，计算的时候只需修改上标即可，比如$g'(z^{(2)})=a^{(2)}·*(1-a^{(2)})$

如果我们不考虑正则化的话，可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}
$$
算法步骤：

对于$m$个训练集

初始化$\triangle_{ij}^{(l)}=0$

对每个训练集$i=1$ to $m$：

​	set $a^{(1)}=x^{(i)}$

​	正向传播计算出每一层的值

​	使用$y^{(i)}$来计算$\delta^{(L)}=a^{(L)}-y^{(i)}$

​	反向传播计算出每一个$\delta$

​	累加：$\triangle_{ij}^{(l)}:=\triangle_{ij}^{(l)}+a_j^{(l)}\delta_{i}^{(l+1)}$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}\quad if \quad j\neq0$

$D_{ij}^{(l)}:=\frac{1}{m}\triangle_{ij}^{(l)}\quad if \quad j=0$

可以证明：
$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}
$$

### 理解反向传播

![](./res/27.png)

### 小细节：展开参数（矩阵到向量）

在我们求$minJ(\Theta)$的过程中，我们通常会用到matlab中更为高级的优化算法，但是在算法的接口中我们只有一个$\Theta$参数的传入，神经网络中我们却有多个$\Theta^{(l)}$，因此我们需要将这些$\Theta$展开为一个向量传入算法中。

举个例子：

![](./res/28.png)

把矩阵展开为一个向量：

```matlab
thetaVec = [ Theta1(:); Theta2(:); Theta3(:) ];
DVec = [D1(:); D2(:); D3(:)];
```

恢复向量到矩阵：

```matlab
Theta1 = reshape(thetaVec(1:110),10,11);
Theta2 = reshape(thetaVec(111:220),10,11);
Theta3 = reshape(thetaVec(221:231),1,11);
```

### 梯度检测

反向传播的实现中，经常会出现一些bug，因此我们需要一种检验方式来检测反向传播是否正常运作。

![](./res/29.png)

我们使用偏导数的定义来求取梯度，与反向传播的对比，如果一致则继续使用反向传播，**记住一定要取消梯度检测的代码，梯度检测的时间复杂度远高于反向传播。**

### 随机初始化

在前面的线性回归和逻辑回归中，我们通常为$\Theta$取初始值为0，但是在反向传播中，我们不能将值取为0。

在反向传播中，$\Theta$表示权重，在反向传播的过程中，如果所有的权重一致，则结点的计算值就会一致，会造成大量的冗余。

因此我们需要使用随机数来初始化$\Theta$：

```matlab
Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSiLON;
```

这样会使$\Theta$的值处于$[-\epsilon,+\epsilon]$之间

### 总结

#### 选择神经网络结构

输入层：数量为特征的种类数

输出层：分类数

较为理想的结构：1层隐藏层，如果大于一层，最好隐藏层的结点数都相等。（一般来说层数越多越好，但是学习所耗费的时间更多）

### 训练神经网络

1. 随机初始化权重
2. 正向传播计算出每个结点的值
3. 计算$J(\Theta)$
4. 反向传播计算梯度
5. 使用梯度检测来检测反向传播是否正确
6. 使用高级优化算法最小化$J(\Theta)$

